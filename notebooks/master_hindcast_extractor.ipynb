{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master hindcast extractor\n",
    "\n",
    "My latest code for optimized extraction of long hindcast fields\n",
    "\n",
    "The local extraction method uses [Dask](https://dask.org) and [xarray.open_mfdataset](http://xarray.pydata.org/en/stable/io.html#reading-multi-file-datasets) based on Doug's optimized workflow in [dask-expts](https://nbviewer.jupyter.org/urls/bitbucket.org/salishsea/analysis-doug/raw/default/notebooks/dask-expts/dask_expts.ipynb). \\\n",
    "*I get about 1 h extraction time per year of hourly surface fields across 12 Salish workers using this method.*\n",
    "\n",
    "The ERDDAP method is much slower but publically accessible and requires no user-side CPU resources. \\\n",
    "*Extraction time is about 12 h per year of hourly surface fields.*\n",
    "\n",
    "Both methods extract in monthy chunks and save to yearly files. The code is designed to combine variables on the *same* model grid (temperature and nitrate for example) but not across different grids (so velocity separate from temperate separate from wind fields, run multiple times to extract these separately).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "from contextlib import ExitStack\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from salishsea_tools import grid_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Functions\n",
    "\n",
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prefix(date, paths, model='NEMO', res='1h'):\n",
    "    \"\"\"Construct path prefix for local SalishSeaCast results given date object\n",
    "    and paths dict. e.g.,\n",
    "    /results/SalishSea/hindcast.201905/ddmmmyy/SalishSea_1h_yyyymmdd_yyyymmdd\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'NEMO':\n",
    "        if (date.year >= 2013) or (date.year <= 2016): path = paths['NEMO1']\n",
    "        else: path = paths['NEMO2']\n",
    "        datestr = '_'.join(np.repeat(date.strftime('%Y%m%d'), 2))\n",
    "        prefix = os.path.join(path, date.strftime('%d%b%y').lower(), f'SalishSea_{res}_{datestr}')\n",
    "    elif model == 'HRDPS':\n",
    "        if date > datetime(2014, 9, 11):\n",
    "            prefix = os.path.join(paths['HRDPS'], 'operational', 'ops_' + date.strftime('y%Ym%md%d'))\n",
    "        else:\n",
    "            prefix = os.path.join(paths['HRDPS'], 'gemlam', 'gemlam_' + date.strftime('y%Ym%md%d'))\n",
    "    else:\n",
    "        return ValueError(f'Unknown model: {model}')\n",
    "\n",
    "    return prefix\n",
    "\n",
    "\n",
    "def load_paths():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    paths = {\n",
    "        'NEMO1': '/results/SalishSea/hindcast.201905',\n",
    "        'NEMO2': '/results2/SalishSea/nowcast-green.201905',\n",
    "        'HRDPS': '/results/forcing/atmospheric/GEM2.5',\n",
    "        'erddap': 'https://salishsea.eos.ubc.ca/erddap/griddap',\n",
    "        'out': '/ocean/bmoorema/research/MEOPAR/analysis-ben/data/SalishSeaCast',\n",
    "    }\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_netCDF_keys(filesystem='errdap'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # NetCDF file keys master dict\n",
    "    if filesystem == 'errdap':\n",
    "        key_dict = {\n",
    "            'temperature': 'g3DTracer',\n",
    "            'salinity': 'g3DTracer',\n",
    "            'nitrate': 'g3DBiology',\n",
    "            'uVelocity': 'g3DuGrid',\n",
    "            'vVelocity': 'g3DvGrid',\n",
    "            'u_wind': 'aSurfaceAtmosphere',\n",
    "            'v_wind': 'aSurfaceAtmosphere',\n",
    "        }\n",
    "    elif filesystem == 'local':\n",
    "        key_dict = {\n",
    "            'votemper': 'grid_T',\n",
    "            'vosaline': 'grid_T',\n",
    "            'nitrate': 'ptrc_T',\n",
    "            'vozocrtx': 'grid_U',\n",
    "            'vomecrty': 'grid_V',\n",
    "            'u_wind': 'ops',\n",
    "            'v_wind': 'ops',\n",
    "        }\n",
    "        \n",
    "    return key_dict\n",
    "\n",
    "\n",
    "def extract_variables(\n",
    "    data_vars, ds, variables, key_dict, dates=[None], dim='time',\n",
    "    indices={'gridX': slice(None), 'gridY': slice(None)}\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Define time index dict\n",
    "    tindex = {dim: slice(*dates)}\n",
    "    \n",
    "    # Loop through variables\n",
    "    for var in variables:\n",
    "\n",
    "        # Initialize data array\n",
    "        if not var in data_vars:\n",
    "            data_vars[var] = ds[key_dict[var]][var].isel(indices).sel(tindex).load()\n",
    "\n",
    "        # Concatenate data arrays\n",
    "        else:\n",
    "            data_vars[var] = xr.concat(\n",
    "                (data_vars[var], ds[key_dict[var]][var].isel(indices).sel(tindex).load()), dim=dim,\n",
    "            )\n",
    "            \n",
    "    return data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hindcast(\n",
    "    client, variables, daterange, mask=None, res='1h', version='19-05',\n",
    "    model='NEMO', filesystem='local', n_workers=12,\n",
    "    indices={'x': slice(None), 'y': slice(None)},\n",
    "    cnk={'time_counter': None, 'y': None, 'x': None},\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare variable definitions\n",
    "    years, months, days = [[getattr(date, key) for date in daterange] for key in ['year', 'month', 'day']]\n",
    "    paths = load_paths()\n",
    "    key_dict = load_netCDF_keys(filesystem=filesystem)\n",
    "    if any(var in variables for var in ['u_wind', 'v_wind']): res, version = '', '1'\n",
    "    ds, keys = {}, list(set([key_dict[var] for var in variables]))\n",
    "    encoding = dict(zip(variables, np.repeat({'zlib': True}, len(variables))))\n",
    "    prefix_out = os.path.join(paths['out'], filesystem, model)\n",
    "    \n",
    "    # Initiate loading protocol based on filesystem\n",
    "    if filesystem == 'local':\n",
    "        prefix_out = f'{prefix_out}_{key_dict[variables[0]]}_'\n",
    "        meshmask = (['y', 'x'], mask[indices['y'], indices['x']])\n",
    "        kwg = {'combine': 'nested', 'concat_dim': 'time_counter', 'parallel': True}\n",
    "    elif filesystem == 'errdap':\n",
    "        prefix_out = f'{prefix_out}_{key_dict[variables[0]][1:]}_'\n",
    "        meshmask = (['gridY', 'gridX'], mask[indices['gridY'], indices['gridX']])\n",
    "        for key in keys:\n",
    "            ds[key] = xr.open_dataset(os.path.join(paths['erddap'], f'ubcSS{key}Fields{res}V{version}'))\n",
    "        attrs = ds[key_dict[variables[0]]].attrs\n",
    "    else:\n",
    "        raise ValueError(f'Unknown filesystem: {filesystem}')\n",
    "    \n",
    "    # Loop through years\n",
    "    for year in range(years[0], years[1] + 1):\n",
    "        \n",
    "        # Initialize data_vars dict and parse months\n",
    "        data_vars = {}\n",
    "        monthday = [[1, 1], [12, 31]]\n",
    "        monthspan = [1, 13]\n",
    "        if year == years[0]: monthspan[0] = months[0]\n",
    "        if year == years[1]: monthspan[1] = months[1] + 1\n",
    "            \n",
    "        # Extract data month by month\n",
    "        for month in tqdm(range(*monthspan), desc=f'Loading {year}'):\n",
    "\n",
    "            # Parse daterange\n",
    "            day, monthdays = 1, monthrange(year, month)[1]\n",
    "            if (year == years[0]) and (month == months[0]):\n",
    "                day = days[0]\n",
    "                monthdays = monthdays - day + 1\n",
    "                monthday[0] = [month, day]\n",
    "            if (year == years[1]) and (month == months[1]):\n",
    "                monthdays = days[1]\n",
    "                monthday[1] = [month, monthdays]\n",
    "            startdate = datetime(year, month, day)\n",
    "            \n",
    "            # Load variables from local filesystem using xarray.mfdataset and dask\n",
    "            if filesystem == 'local':\n",
    "                prefixes = [\n",
    "                    make_prefix(startdate + timedelta(days=day), paths, model=model, res=res)\n",
    "                    for day in range(monthdays)\n",
    "                ]\n",
    "                with ExitStack() as stack:\n",
    "                    for key in keys:\n",
    "                        tag = key\n",
    "                        if model == 'HRDPS': tag = ''\n",
    "                        flist = [prefix + tag + '.nc' for prefix in prefixes]\n",
    "                        ds[key] = stack.enter_context(xr.open_mfdataset(flist, chunks=cnk, **kwg))\n",
    "                    attrs = ds[key_dict[variables[0]]].attrs\n",
    "                    data_vars = extract_variables(data_vars, ds, variables, key_dict, dim='time_counter', indices=indices)\n",
    "            \n",
    "            # Load variables from ERDDAP using specified month range\n",
    "            elif filesystem == 'errdap':\n",
    "                dates = [startdate, startdate + timedelta(monthdays)]\n",
    "                data_vars = extract_variables(data_vars, ds, variables, key_dict, dates=dates, indices=indices)\n",
    "            \n",
    "            # Raise ValueError if filesystem is defined incorrectly\n",
    "            else:\n",
    "                raise ValueError(f'Unknown filesystem: {filesystem}')\n",
    "\n",
    "        # Save year's worth of data as netCDF file\n",
    "        if mask is not None: data_vars['meshmask'] = meshmask\n",
    "        datestr = '_'.join(datetime(year, *md).strftime('%Y%m%d') for md in monthday)\n",
    "        with xr.Dataset(data_vars=data_vars, attrs=attrs) as obj:\n",
    "            obj.to_netcdf(prefix_out + datestr + '.nc', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Perform the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indices and variables\n",
    "paths = load_paths()\n",
    "#indices = {'gridX': slice(115, 360), 'gridY': slice(310, 788), 'depth': 0} # NEMO\n",
    "#cnk = {'time_counter': 3, 'deptht': 40*3, 'y': 898*3, 'x': 398*3}\n",
    "indices = {'x': slice(100, 170), 'y': slice(110, 190)} # HRDPS\n",
    "cnk = {'time_counter': 3, 'y': 266*3, 'x': 256*3}\n",
    "variables = ['u_wind', 'v_wind']\n",
    "mask_NEMO = xr.open_dataset('/data/bmoorema/MEOPAR/grid/mesh_mask201702.nc')\n",
    "grid_NEMO = xr.open_dataset('/data/bmoorema/MEOPAR/grid/coordinates_seagrid_SalishSea201702.nc', decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68096/68096 [06:29<00:00, 174.65it/s]\n",
      "100%|██████████| 68096/68096 [06:28<00:00, 175.20it/s]\n",
      "100%|██████████| 68096/68096 [06:33<00:00, 173.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build HRDPS masks\n",
    "mask_HRDPS = {}\n",
    "dates = [datetime(2011, 9, 21), datetime(2011, 9, 22), datetime(2014, 9, 12)]\n",
    "for date in dates:\n",
    "    with xr.open_dataset(make_prefix(date, paths, model='HRDPS') + '.nc') as grid_HRPDS:\n",
    "        mask_HRDPS[date] = grid_tools.build_HRDPS_mask(grid_HRPDS, grid_NEMO, mask_NEMO.tmask[0, 0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=12, threads_per_worker=2, processes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d23b13310d944239f0a88aa8f0a6e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2010', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fd2bceea8f4e019806bea412c8eaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2011', max=9, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a531653070ce453a928c0b2d552892f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2011', max=4, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9cd94ec929460493cf6c5e123b07ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2012', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f11b6da9234da08222dfa8c4a95153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2013', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4abb5eeeb841d1a6450e7856a3c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2014', max=9, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a9dabbaf243db989cb5eaec274caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2014', max=4, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df6effda965413fa68c181b804eeaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2015', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6586a7672d1481a842486bd78ae4f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2016', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1379de0e44d2483f6c6b2ab39b929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2017', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fe931fb6db4e07a92cfe9b72a06424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2018', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3548cf7b474528b0c42881171a1a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading 2019', max=12, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dateranges = [\n",
    "    (datetime(2010, 1, 1), dates[0]),\n",
    "    (dates[1], dates[2]-timedelta(days=1)),\n",
    "    (dates[2], datetime(2019, 12, 31)),\n",
    "]\n",
    "for daterange, maskdate in zip(dateranges, mask_HRDPS):\n",
    "    extract_hindcast(client, variables, daterange, mask=mask_HRDPS[maskdate], model='HRDPS', indices=indices, cnk=cnk)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
